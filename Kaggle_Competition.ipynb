{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kaggle Comptetition: Black Friday Sale Prediction\n",
    "\n",
    "## General Instruction:\n",
    "\n",
    "__Task__: The objective to predict the primary product category given other features of the product. You may also create your own features.<br>\n",
    "\n",
    "__Metrics__: The evaluation metric for this competition is Accuracy.<br>\n",
    "\n",
    "__Other metrics (optional)__While you are working on the problem, you should also check the precision and recall of your models. However, this is\n",
    "for your learning, and will not be considered in the evaluation.<br>\n",
    "\n",
    "__Submission Format__<br>\n",
    "The solution file will be a CSV file consisting of Product_ID and your predicted class. It should contain two columns: Product_ID and Product_Category_1.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _import neccessary library_\n",
    " - Some universally basic libraries are: numpy, pandas, matplotlib\n",
    " - standard scaler is to normalize the data\n",
    " - Notice, in this project, we use get_dummies method from pandas, which does the job of both lable encoder and onehotencoder\n",
    " - Some machine learning library in used are Random Forest, Bagging, AdaBoost, Voting, Logistic Regression, Decision Tree, Neural Networks\n",
    " - Some libraries for model selection are Cross Validation and Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1061,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Exploring the data_:\n",
    " - The data in used consist of the following variables: Gender, Age, Occupation, City Category, Marital Status, Stay_In_Current_City_Years, Product_Category_2, Product_Category_3, Puchase, User_ID, Product_ID, Product_Category_1 <br>\n",
    "\n",
    " - Our target for this project is Product_Category_1\n",
    " - At first, all of the categories were used as features. However, we discovered that some features hold more predictive power while others do less so. To be more specfic, when changing from all variable to just a selective few: Purchase, Product_Category_2, Product_Category_3, we archieve  a significant improvment in term of accuracy of 6 percent, and when using only Purchase and Product_Category_2, we earned another 1% increment in accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThis is the data with all features. \\nThis cell is commented out and the only data in used\\nwill be from Category2 and Purchase\\n'"
      ]
     },
     "execution_count": 1196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "This is the data with all features. \n",
    "This cell is commented out and the only data in used\n",
    "will be from Category2 and Purchase\n",
    "'''\n",
    "# # read in the data\n",
    "# df = pd.read_csv('black_friday_data_kaggle.csv')\n",
    "\n",
    "# # one hot encoding the categorical data\n",
    "# df = pd.get_dummies(data=df, columns=['Gender', 'Age', 'Occupation',\n",
    "#                                    'City_Category', 'Marital_Status',\n",
    "#                                    'Stay_In_Current_City_Years',])\n",
    "# df = df.drop(['Product_Category_2', 'Product_Category_3'], axis = 1)\n",
    "\n",
    "# #drop missing data\n",
    "# df = df.dropna()\n",
    "\n",
    "\n",
    "# # get the mean value of the data so that there is one one row for each product id\n",
    "# df = df.groupby(['Product_ID', 'Product_Category_1'])[['Gender_F', 'Gender_M', \n",
    "#                                                   'Age_0-17', 'Age_18-25', 'Age_26-35','Age_36-45',\n",
    "#                                                   'Age_46-50', 'Age_51-55', 'Age_55+',\n",
    "#                                                   'Occupation_0', 'Occupation_1', 'Occupation_2',\n",
    "#                                                   'Occupation_3', 'Occupation_4', 'Occupation_5',\n",
    "#                                                   'Occupation_6', 'Occupation_7', 'Occupation_8',\n",
    "#                                                   'Occupation_9', 'Occupation_10', 'Occupation_11',\n",
    "#                                                   'Occupation_12', 'Occupation_13', 'Occupation_14',\n",
    "#                                                   'Occupation_15', 'Occupation_16', 'Occupation_17',\n",
    "#                                                   'Occupation_18', 'Occupation_19', 'Occupation_20',\n",
    "#                                                   'City_Category_A', 'City_Category_B', 'City_Category_C', \n",
    "#                                                   'Marital_Status_0', 'Marital_Status_1',\n",
    "#                                                   'Stay_In_Current_City_Years_0', 'Stay_In_Current_City_Years_1',\n",
    "#                                                   'Stay_In_Current_City_Years_2', 'Stay_In_Current_City_Years_3',\n",
    "#                                                   'Stay_In_Current_City_Years_4+', 'Purchase']].median()\n",
    "\n",
    "# df.reset_index(inplace=True)\n",
    "\n",
    "# #normalize the data\n",
    "# sc = StandardScaler()\n",
    "# df.loc[:, ~df.columns.isin(['User_ID', 'Product_ID', 'Product_Category_1'])] = sc.fit_transform(df.loc[:, ~df.columns.isin(['User_ID', 'Product_ID', 'Product_Category_1'])])\n",
    "# df.shape # (3623, 43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Feature Engineering_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3623, 21)"
      ]
     },
     "execution_count": 1188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "################## ************* ####################\n",
    "# read in the data\n",
    "df = pd.read_csv('black_friday_data_kaggle.csv', index_col=0)\n",
    "\n",
    "# drop unessary variables with low to none predictive power to Product_Category_1\n",
    "df = df.drop(['Gender', 'Age', 'Occupation',\n",
    "              'City_Category', 'Marital_Status',\n",
    "              'Stay_In_Current_City_Years', 'Product_Category_3'], axis = 1)\n",
    "\n",
    "# Notice, in this case we don't drop missing data sincce\n",
    "# it would reduce our test set to be less than 1207 rows\n",
    "\n",
    "# one hot encoding the categorical data\n",
    "df = pd.get_dummies(data=df, columns=['Product_Category_2'])\n",
    "\n",
    "\n",
    "# get the mean value of the data so that there is one one row for each product id\n",
    "df = df.groupby(['Product_ID', 'Product_Category_1']).median()\n",
    "\n",
    "# reset the index formed by groupbe,\n",
    "# the 2 index value Product_ID and Product_Categorical_1\n",
    "# will become two columns\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "#normalize the data\n",
    "sc = StandardScaler()\n",
    "df.loc[:, ~df.columns.isin(['User_ID', 'Product_ID', 'Product_Category_1'])] = sc.fit_transform(df.loc[:, ~df.columns.isin(['User_ID', 'Product_ID', 'Product_Category_1'])])\n",
    "df.shape "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Split the Data_:\n",
    "The data shall be split into training and testing based on whether their Product_Category_1 is -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1207,)"
      ]
     },
     "execution_count": 1189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the data into train and predicting set based on value of Product_Category_1\n",
    "train = df.loc[df.Product_Category_1 != -1,:]\n",
    "x_train = train.loc[:,~train.columns.isin(['Product_ID', 'Product_Category_1'])]\n",
    "y_train = train.Product_Category_1\n",
    "\n",
    "test = df.loc[df.Product_Category_1 == -1,:]\n",
    "x_test = test.loc[:,~test.columns.isin(['Product_ID', 'Product_Category_1'])]\n",
    "y_test = test.Product_Category_1\n",
    "y_test.shape #(1207,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3623, 21)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "NOTE TO TEAM: This cell is about the original full dataset.\n",
    "I use it to test our models. Don't use it in your training.\n",
    "Otherwise, you will get a 100% accuracy\n",
    "Don't mention this in the write up either. The purpose of \n",
    "using this is only to save time from keep uploading file to Kaggle\n",
    "\n",
    "It is processed in the exact same way with the \"legitimate\" dataset above\n",
    "\n",
    "-----------------------------------------------------------------\n",
    "\n",
    "this is the original data set, we picked out the \n",
    "records that have product id in the test dataset\n",
    "this new data provide us true target of the test set,\n",
    "which were changed to -1 in the test set\n",
    "'''\n",
    "\n",
    "# original= pd.read_csv('BlackFriday.csv')\n",
    "# original = pd.get_dummies(data=original, columns=['Gender', 'Age', 'Occupation',\n",
    "#                                    'City_Category', 'Marital_Status',\n",
    "#                                    'Stay_In_Current_City_Years',])\n",
    "# original = original.drop(['Product_Category_2', 'Product_Category_3'], axis = 1)\n",
    "# original = original.dropna()\n",
    "\n",
    "# original = original.groupby(['Product_ID', 'Product_Category_1'])[['Gender_F', 'Gender_M', \n",
    "#                                                   'Age_0-17', 'Age_18-25', 'Age_26-35','Age_36-45',\n",
    "#                                                   'Age_46-50', 'Age_51-55', 'Age_55+',\n",
    "#                                                   'Occupation_0', 'Occupation_1', 'Occupation_2',\n",
    "#                                                   'Occupation_3', 'Occupation_4', 'Occupation_5',\n",
    "#                                                   'Occupation_6', 'Occupation_7', 'Occupation_8',\n",
    "#                                                   'Occupation_9', 'Occupation_10', 'Occupation_11',\n",
    "#                                                   'Occupation_12', 'Occupation_13', 'Occupation_14',\n",
    "#                                                   'Occupation_15', 'Occupation_16', 'Occupation_17',\n",
    "#                                                   'Occupation_18', 'Occupation_19', 'Occupation_20',\n",
    "#                                                   'City_Category_A', 'City_Category_B', 'City_Category_C', \n",
    "#                                                   'Marital_Status_0', 'Marital_Status_1',\n",
    "#                                                   'Stay_In_Current_City_Years_0', 'Stay_In_Current_City_Years_1',\n",
    "#                                                   'Stay_In_Current_City_Years_2', 'Stay_In_Current_City_Years_3',\n",
    "#                                                   'Stay_In_Current_City_Years_4+', 'Purchase']].median()\n",
    "\n",
    "############ ******** #############\n",
    "\n",
    "\n",
    "original= pd.read_csv('BlackFriday.csv')\n",
    "\n",
    "\n",
    "original = original.drop(['Gender', 'Age', 'Occupation',\n",
    "                          'City_Category', 'Marital_Status',\n",
    "                          'Stay_In_Current_City_Years', 'Product_Category_3'], axis = 1)\n",
    "\n",
    "# original = original.dropna()\n",
    "\n",
    "original = pd.get_dummies(data=original, columns=['Product_Category_2'])\n",
    "\n",
    "\n",
    "\n",
    "original = original.groupby(['Product_ID', 'Product_Category_1']).median()\n",
    "\n",
    "\n",
    "\n",
    "original.reset_index(inplace=True)\n",
    "\n",
    "print(original.shape)\n",
    "\n",
    "\n",
    "\n",
    "# original.dropna()\n",
    "original.loc[:, ~original.columns.isin(['Product_ID', 'Product_Category_1'])] = sc.fit_transform(original.loc[:, ~original.columns.isin(['Product_ID', 'Product_Category_1'])])\n",
    "\n",
    "\n",
    "x_original = original.loc[:,~original.columns.isin(['Product_ID', 'Product_Category_1'])]\n",
    "y_original = original.Product_Category_1 \n",
    "\n",
    "# real_test = original.loc[original.Product_ID.isin(test.Product_ID), ['Product_ID', 'Product_Category_1']]\n",
    "real_y_test = original.loc[original.Product_ID.isin(test.Product_ID), ['Product_Category_1']]\n",
    "real_x_test = original.loc[original.Product_ID.isin(test.Product_ID), ~original.columns.isin(['Product_ID', 'Product_Category_1'])]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _First Model_:  Random Forest\n",
    "\n",
    " - We apply random forest to be our first technic due to its conceptual simplicty.\n",
    " - First, we use Grid Search, which bascially provide a systematic way to train our model with multiple different combinations of hyper parameters.\n",
    " - Also, Grid Search can deploy Cross Validation and determine mean accuary\n",
    " - From this, we will narrow down our choice for hyper parameter and fit it later\n",
    " - Random Forest provide good result. But we will soon have better result from other technics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.829341</td>\n",
       "      <td>0.059570</td>\n",
       "      <td>{'n_estimators': 60}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.820359</td>\n",
       "      <td>0.054512</td>\n",
       "      <td>{'n_estimators': 61}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.829341</td>\n",
       "      <td>0.052714</td>\n",
       "      <td>{'n_estimators': 62}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.814371</td>\n",
       "      <td>0.055175</td>\n",
       "      <td>{'n_estimators': 63}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.820359</td>\n",
       "      <td>0.077548</td>\n",
       "      <td>{'n_estimators': 64}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.045487</td>\n",
       "      <td>{'n_estimators': 65}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.826347</td>\n",
       "      <td>0.064619</td>\n",
       "      <td>{'n_estimators': 66}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.832335</td>\n",
       "      <td>0.051307</td>\n",
       "      <td>{'n_estimators': 67}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.823353</td>\n",
       "      <td>0.059360</td>\n",
       "      <td>{'n_estimators': 68}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.835329</td>\n",
       "      <td>0.060629</td>\n",
       "      <td>{'n_estimators': 69}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.829341</td>\n",
       "      <td>0.043568</td>\n",
       "      <td>{'n_estimators': 70}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.850299</td>\n",
       "      <td>0.064212</td>\n",
       "      <td>{'n_estimators': 71}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.835329</td>\n",
       "      <td>0.049470</td>\n",
       "      <td>{'n_estimators': 72}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.823353</td>\n",
       "      <td>0.055234</td>\n",
       "      <td>{'n_estimators': 73}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.826347</td>\n",
       "      <td>0.060201</td>\n",
       "      <td>{'n_estimators': 74}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.826347</td>\n",
       "      <td>0.045444</td>\n",
       "      <td>{'n_estimators': 75}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.838323</td>\n",
       "      <td>0.051816</td>\n",
       "      <td>{'n_estimators': 76}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.817365</td>\n",
       "      <td>0.051431</td>\n",
       "      <td>{'n_estimators': 77}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.829341</td>\n",
       "      <td>0.066057</td>\n",
       "      <td>{'n_estimators': 78}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.829341</td>\n",
       "      <td>0.056728</td>\n",
       "      <td>{'n_estimators': 79}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score                params\n",
       "0          0.829341        0.059570  {'n_estimators': 60}\n",
       "1          0.820359        0.054512  {'n_estimators': 61}\n",
       "2          0.829341        0.052714  {'n_estimators': 62}\n",
       "3          0.814371        0.055175  {'n_estimators': 63}\n",
       "4          0.820359        0.077548  {'n_estimators': 64}\n",
       "5          0.832335        0.045487  {'n_estimators': 65}\n",
       "6          0.826347        0.064619  {'n_estimators': 66}\n",
       "7          0.832335        0.051307  {'n_estimators': 67}\n",
       "8          0.823353        0.059360  {'n_estimators': 68}\n",
       "9          0.835329        0.060629  {'n_estimators': 69}\n",
       "10         0.829341        0.043568  {'n_estimators': 70}\n",
       "11         0.850299        0.064212  {'n_estimators': 71}\n",
       "12         0.835329        0.049470  {'n_estimators': 72}\n",
       "13         0.823353        0.055234  {'n_estimators': 73}\n",
       "14         0.826347        0.060201  {'n_estimators': 74}\n",
       "15         0.826347        0.045444  {'n_estimators': 75}\n",
       "16         0.838323        0.051816  {'n_estimators': 76}\n",
       "17         0.817365        0.051431  {'n_estimators': 77}\n",
       "18         0.829341        0.066057  {'n_estimators': 78}\n",
       "19         0.829341        0.056728  {'n_estimators': 79}"
      ]
     },
     "execution_count": 1141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_trees = list(range(60,80))\n",
    "param_grid = dict(n_estimators = num_trees)\n",
    "grid = GridSearchCV(RandomForestClassifier(), param_grid, cv = 10, return_train_score=False)\n",
    "grid.fit(x_train, y_train)\n",
    "rf_result = pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/cross_validation.py:553: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of labels for any class cannot be less than n_folds=10.\n",
      "  % (min_labels, self.n_folds)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8251048935956906\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6876553438276719"
      ]
     },
     "execution_count": 1191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first attempt: fit the random forest\n",
    "clf_rf = RandomForestClassifier(n_estimators=70,random_state=0)\n",
    "clf_rf.fit(X=x_train, y=y_train)\n",
    "cv_scores = cross_val_score(clf_rf, X=x_train, y=y_train, cv = 10, scoring='accuracy')\n",
    "print(np.mean(cv_scores))\n",
    "clf_rf.score(real_x_test, real_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1207, 2)"
      ]
     },
     "execution_count": 1192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predict the data and write result to csv\n",
    "# pred = clf_rf.predict(x_test)\n",
    "# result = pd.DataFrame({'Product_ID' : test.Product_ID, 'Product_Category_1' : pred})\n",
    "# result = result[['Product_ID', 'Product_Category_1']]\n",
    "# result.to_csv('Prediction.csv', index=False)\n",
    "# result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Bagging_\n",
    "To our surprise, a method that is similar to random forest can actually improve our accuracy to to its versatility.\n",
    " - We first use gridsearch to determine a optimal choice for parameter for the decision tree\n",
    " - Then, once again we use gridsearch to determine the choice of max_features and n_esitimators for the bagging algorithm\n",
    " - Finally, we fir the model into the training data and archive a high score based on Kaggle evalution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.602649</td>\n",
       "      <td>0.027726</td>\n",
       "      <td>{'max_depth': 4, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.746275</td>\n",
       "      <td>0.025476</td>\n",
       "      <td>{'max_depth': 5, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.795944</td>\n",
       "      <td>0.027529</td>\n",
       "      <td>{'max_depth': 6, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.796772</td>\n",
       "      <td>0.017249</td>\n",
       "      <td>{'max_depth': 7, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.801325</td>\n",
       "      <td>0.020834</td>\n",
       "      <td>{'max_depth': 8, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.808775</td>\n",
       "      <td>0.020007</td>\n",
       "      <td>{'max_depth': 9, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.808361</td>\n",
       "      <td>0.026896</td>\n",
       "      <td>{'max_depth': 10, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.817053</td>\n",
       "      <td>0.029192</td>\n",
       "      <td>{'max_depth': 11, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.810430</td>\n",
       "      <td>0.031391</td>\n",
       "      <td>{'max_depth': 12, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.805877</td>\n",
       "      <td>0.027892</td>\n",
       "      <td>{'max_depth': 13, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.807947</td>\n",
       "      <td>0.030285</td>\n",
       "      <td>{'max_depth': 14, 'criterion': 'gini'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.628725</td>\n",
       "      <td>0.045191</td>\n",
       "      <td>{'max_depth': 4, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.776490</td>\n",
       "      <td>0.033146</td>\n",
       "      <td>{'max_depth': 5, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.823675</td>\n",
       "      <td>0.016695</td>\n",
       "      <td>{'max_depth': 6, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.827815</td>\n",
       "      <td>0.020713</td>\n",
       "      <td>{'max_depth': 7, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.824917</td>\n",
       "      <td>0.014982</td>\n",
       "      <td>{'max_depth': 8, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.824089</td>\n",
       "      <td>0.021075</td>\n",
       "      <td>{'max_depth': 9, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.819536</td>\n",
       "      <td>0.022382</td>\n",
       "      <td>{'max_depth': 10, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.810017</td>\n",
       "      <td>0.030539</td>\n",
       "      <td>{'max_depth': 11, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.804636</td>\n",
       "      <td>0.028821</td>\n",
       "      <td>{'max_depth': 12, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.802566</td>\n",
       "      <td>0.030982</td>\n",
       "      <td>{'max_depth': 13, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.800497</td>\n",
       "      <td>0.029611</td>\n",
       "      <td>{'max_depth': 14, 'criterion': 'entropy'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_test_score  std_test_score                                     params\n",
       "0          0.602649        0.027726      {'max_depth': 4, 'criterion': 'gini'}\n",
       "1          0.746275        0.025476      {'max_depth': 5, 'criterion': 'gini'}\n",
       "2          0.795944        0.027529      {'max_depth': 6, 'criterion': 'gini'}\n",
       "3          0.796772        0.017249      {'max_depth': 7, 'criterion': 'gini'}\n",
       "4          0.801325        0.020834      {'max_depth': 8, 'criterion': 'gini'}\n",
       "5          0.808775        0.020007      {'max_depth': 9, 'criterion': 'gini'}\n",
       "6          0.808361        0.026896     {'max_depth': 10, 'criterion': 'gini'}\n",
       "7          0.817053        0.029192     {'max_depth': 11, 'criterion': 'gini'}\n",
       "8          0.810430        0.031391     {'max_depth': 12, 'criterion': 'gini'}\n",
       "9          0.805877        0.027892     {'max_depth': 13, 'criterion': 'gini'}\n",
       "10         0.807947        0.030285     {'max_depth': 14, 'criterion': 'gini'}\n",
       "11         0.628725        0.045191   {'max_depth': 4, 'criterion': 'entropy'}\n",
       "12         0.776490        0.033146   {'max_depth': 5, 'criterion': 'entropy'}\n",
       "13         0.823675        0.016695   {'max_depth': 6, 'criterion': 'entropy'}\n",
       "14         0.827815        0.020713   {'max_depth': 7, 'criterion': 'entropy'}\n",
       "15         0.824917        0.014982   {'max_depth': 8, 'criterion': 'entropy'}\n",
       "16         0.824089        0.021075   {'max_depth': 9, 'criterion': 'entropy'}\n",
       "17         0.819536        0.022382  {'max_depth': 10, 'criterion': 'entropy'}\n",
       "18         0.810017        0.030539  {'max_depth': 11, 'criterion': 'entropy'}\n",
       "19         0.804636        0.028821  {'max_depth': 12, 'criterion': 'entropy'}\n",
       "20         0.802566        0.030982  {'max_depth': 13, 'criterion': 'entropy'}\n",
       "21         0.800497        0.029611  {'max_depth': 14, 'criterion': 'entropy'}"
      ]
     },
     "execution_count": 1070,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_dt = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "parameters = [{'max_depth': list(np.arange(4,15)),\n",
    "              'criterion': ['gini', 'entropy']}]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = clf_dt,\n",
    "                           param_grid=parameters,\n",
    "                           scoring='accuracy',\n",
    "                           cv = 10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "rf_result = pd.DataFrame(grid_search.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\n",
    "\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/model_selection/_split.py:597: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=10.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.865480</td>\n",
       "      <td>0.018904</td>\n",
       "      <td>{'max_features': 0.8, 'n_estimators': 48}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.023222</td>\n",
       "      <td>{'max_features': 0.8, 'n_estimators': 49}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.861755</td>\n",
       "      <td>0.023749</td>\n",
       "      <td>{'max_features': 0.8, 'n_estimators': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.856788</td>\n",
       "      <td>0.025901</td>\n",
       "      <td>{'max_features': 0.8, 'n_estimators': 51}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.858858</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>{'max_features': 0.8, 'n_estimators': 52}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.861755</td>\n",
       "      <td>0.023412</td>\n",
       "      <td>{'max_features': 0.9, 'n_estimators': 48}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.854305</td>\n",
       "      <td>0.024702</td>\n",
       "      <td>{'max_features': 0.9, 'n_estimators': 49}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.859272</td>\n",
       "      <td>0.022137</td>\n",
       "      <td>{'max_features': 0.9, 'n_estimators': 50}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.860099</td>\n",
       "      <td>0.024248</td>\n",
       "      <td>{'max_features': 0.9, 'n_estimators': 51}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.860099</td>\n",
       "      <td>0.023777</td>\n",
       "      <td>{'max_features': 0.9, 'n_estimators': 52}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  std_test_score                                     params\n",
       "0         0.865480        0.018904  {'max_features': 0.8, 'n_estimators': 48}\n",
       "1         0.860927        0.023222  {'max_features': 0.8, 'n_estimators': 49}\n",
       "2         0.861755        0.023749  {'max_features': 0.8, 'n_estimators': 50}\n",
       "3         0.856788        0.025901  {'max_features': 0.8, 'n_estimators': 51}\n",
       "4         0.858858        0.020134  {'max_features': 0.8, 'n_estimators': 52}\n",
       "5         0.861755        0.023412  {'max_features': 0.9, 'n_estimators': 48}\n",
       "6         0.854305        0.024702  {'max_features': 0.9, 'n_estimators': 49}\n",
       "7         0.859272        0.022137  {'max_features': 0.9, 'n_estimators': 50}\n",
       "8         0.860099        0.024248  {'max_features': 0.9, 'n_estimators': 51}\n",
       "9         0.860099        0.023777  {'max_features': 0.9, 'n_estimators': 52}"
      ]
     },
     "execution_count": 1193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bg = BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=8))\n",
    "\n",
    "parameters = [{'max_features': [0.8, 0.9],\n",
    "              'n_estimators': np.arange(48,53)}]\n",
    "\n",
    "grid_search = GridSearchCV(estimator = bg,\n",
    "                           param_grid=parameters,\n",
    "                           scoring='accuracy',\n",
    "                           cv = 10, n_jobs=-1)\n",
    "\n",
    "grid_search.fit(x_train, y_train)\n",
    "\n",
    "rf_result = pd.DataFrame(grid_search.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\n",
    "\n",
    "rf_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.816072908036454"
      ]
     },
     "execution_count": 1194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bagging\n",
    "bg = BaggingClassifier(base_estimator=DecisionTreeClassifier(),\n",
    "                       max_features=0.8,\n",
    "                       n_estimators=50,\n",
    "                       random_state=2)\n",
    "bg.fit(x_train, y_train)\n",
    "bg.score(real_x_test, real_y_test)\n",
    "# the best so far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1207, 2)"
      ]
     },
     "execution_count": 1195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# seeing that the result is the best so far, we write it to csv\n",
    "# predict the data and write result to csv\n",
    "pred = bg.predict(x_test)\n",
    "result = pd.DataFrame({'Product_ID' : test.Product_ID, 'Product_Category_1' : pred})\n",
    "result = result[['Product_ID', 'Product_Category_1']]\n",
    "result.to_csv('Prediction.csv', index=False)\n",
    "result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _Other technics_:\n",
    "We also try on some other models such as:\n",
    "- bagging of Neural Networks\n",
    "- AdaBoosting\n",
    "- Voting\n",
    "- XGBoost\n",
    "\n",
    "To our disappoinment, Bagging of Neural Networks did not improve accuracy. This is likely due to the nature of bagging that is meant for simplier model to avoid overfiiting <br>\n",
    "Boosting with Decision Tree delivered fairly good result but not as strong as Bagging of Tree as shown above\n",
    "Also, it is worth noticing that XGBoost appears to have highly accuarate prediction. However, the result contains empty array which is not acceptable for Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1086,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5642087821043911\n",
      "0.5932062966031483\n"
     ]
    }
   ],
   "source": [
    "# attempt neural networks\n",
    "# build nn model\n",
    "clf_nn = MLPClassifier(solver = 'lbfgs', activation = 'logistic', max_iter=40,\n",
    "                    hidden_layer_sizes = 10, random_state = 0)\n",
    "# fit the nn_model\n",
    "clf_nn.fit(x_train, y_train)\n",
    "\n",
    "# get real accuracy\n",
    "print(clf_nn.score(real_x_test, real_y_test))\n",
    "\n",
    "#apply bagging for neural networks \n",
    "bg = BaggingClassifier(base_estimator=clf_nn, max_features=0.9, n_estimators=55)\n",
    "\n",
    "bg.fit(x_train, y_train)\n",
    "\n",
    "print(bg.score(real_x_test, real_y_test))\n",
    "\n",
    "# after submitting to kaggle, we see that even when bagging nn improve from pure nn,\n",
    "# it still does not beat bagging of Decistion Tree\n",
    "# the reason is due to the relation between ensemble techniques and the simplicity of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1090,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7937033968516984\n"
     ]
    }
   ],
   "source": [
    "# Boosting\n",
    "adb = AdaBoostClassifier(DecisionTreeClassifier(), n_estimators=50, learning_rate=1)\n",
    "adb.fit(x_train, y_train)\n",
    "print(adb.score(real_x_test, real_y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1076,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5741507870753936"
      ]
     },
     "execution_count": 1076,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Voting \n",
    "lr = LogisticRegression()\n",
    "dt = DecisionTreeClassifier()\n",
    "svm = SVC(kernel = 'rbf')\n",
    "evc = VotingClassifier(estimators=[('dt', dt), ('svm', svm), ('lr', lr)])\n",
    "evc.fit(x_train, y_train)\n",
    "evc.score(real_x_test, real_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8044739022369511\n",
      "#########\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "clf_xgb = XGBClassifier(max_depth=10, random_state=0)\n",
    "clf_xgb.fit(x_train, y_train)\n",
    "print(clf_xgb.score(real_x_test, real_y_test))\n",
    "print('#########')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
